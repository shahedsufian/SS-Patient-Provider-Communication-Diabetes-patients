Python 3.13.1 (tags/v3.13.1:0671451, Dec  3 2024, 19:06:28) [MSC v.1942 64 bit (AMD64)] on win32
Type "help", "copyright", "credits" or "license()" for more information.
>>> # -*- coding: utf-8 -*-
... """
... End-to-End MEPS Data Processing and Analysis in Python
... 
... Purpose:
... This script performs a complete data processing and statistical analysis pipeline
... for the MEPS diabetes cohort data.
... 
... Part 1: Data Cleaning and Preparation
... - Reads the four yearly CSV files generated by the initial SAS program.
... - Harmonizes column names across different years.
... - Cleans the data by removing records with invalid or missing values in key columns.
... - Recodes numeric variables into descriptive categorical labels.
... - Creates a composite Patient-Provider Communication (PPC) score.
... - Winsorizes continuous outcome variables at the 99th percentile to handle outliers.
... - Appends the four cleaned yearly datasets into a single master DataFrame.
... 
... Part 2: Statistical Analysis
... - Creates binary dummy variables for outcomes and numeric codes for categorical predictors.
... - Performs survey-weighted descriptive analysis (means and proportions).
... - Runs multiple regression models to assess the relationship between PPC and various
...   health outcomes, including:
...     - Weighted Least Squares (OLS)
...     - Generalized Linear Models (GLM) for cost and count data
...     - Logistic Regression for binary outcomes
...     - Two-Part Models for outcomes with a mix of zeros and positive values
... 
... Required Libraries:
... - pandas: For data manipulation and analysis.
... - numpy: For numerical operations.
... - statsmodels: For statistical modeling and survey-weighted analysis.
... 
... Author: [Your Name]
... Date: [Date]
... """
... 
... # --- 1. SETUP & CONFIGURATION ---
... import pandas as pd
... import numpy as np
... import statsmodels.api as sm
... import statsmodels.formula.api as smf
... from statsmodels.stats.weightstats import DescrStatsW
... 
... # --- IMPORTANT: UPDATE FILE PATHS ---
... # Define file paths for input and output.
... CSV_FILE_PATH = "C:/path/to/your/CSV_Files/"
... OUTPUT_PATH = "C:/path/to/your/output/folder/"
... 
... # Define a dictionary to map the 2019 column names to the standard names
... COLUMN_NAME_MAP_2019 = {
...     "OTHLGSPK": "OTHLANG",
...     "WHTLGSPK": "LANGSPK",
...     "HWELLSPK": "HWELLSPE",
...     "PROVTY2_M18": "PROVTY2",
...     "ADAPPT4": "ADAPPT2",
...     "ADLIST4": "ADLIST2",
...     "ADEXPL4": "ADEXPL2",
...     "ADRESP4": "ADRESP2",
...     "ADPRTM4": "ADPRTM2"
... }
... 
... # --- 2. DATA PROCESSING FUNCTION ---
... 
... def process_meps_year(year, column_map=None):
...     """
...     Reads, cleans, and processes a single year of MEPS data.

    Args:
        year (int): The year of the dataset to process (e.g., 2016).
        column_map (dict, optional): A dictionary to rename columns, typically
                                     for harmonizing the 2019 data.

    Returns:
        pandas.DataFrame: A cleaned and processed DataFrame for the specified year.
    """
    print(f"--- Processing {year} data ---")
    # Load the dataset
    filepath = f"{CSV_FILE_PATH}d_{year}.csv"
    df = pd.read_csv(filepath)

    # Harmonize column names if a map is provided (for 2019)
    if column_map:
        df.rename(columns=column_map, inplace=True)

    # --- Data Cleaning: Remove rows with invalid/missing values ---
    # In MEPS, negative values often represent missing, inapplicable, etc.
    cols_to_clean = ["RTHLTH1", "HAVEUS2", "TREATM2", "DECIDE2", "EXPLOP2",
                     "EMPST1", "ADLIST2", "ADEXPL2", "ADRESP2", "ADPRTM2",
                     "ADAPPT2", "DSCONF3", "OTHLANG", "HIDEG"]
    for col in cols_to_clean:
        # Ensure column is numeric before filtering
        df[col] = pd.to_numeric(df[col], errors='coerce')
        df.dropna(subset=[col], inplace=True)
        df = df[df[col] > 0]

    # --- Drop columns that will not be used in the analysis ---
    cols_to_drop = ["LANGSPK", "TYPEPE2", "HSPLAP2", "WHITPR2", "BLCKPR2",
                    "ASIANP2", "NATAMP2", "PACISP2", "OTHRCP2", "GENDRP2",
                    "PRVSPK2", "HWELLSPE", "HAVEUS2"]
    df.drop(columns=cols_to_drop, inplace=True, errors='ignore')

    # --- Recode variables into descriptive categories ---
    # Create mapping dictionaries for clarity
    recoding_maps = {
        'DSCONF3': {1: 0, 2: 0, 3: 1, 4: 1}, # Recode to Binary (Not confident vs. Confident)
        'RTHLTH1': {1: "Excellent", 2: "Very Good", 3: "Good", 4: "Fair", 5: "Poor"},
        'TREATM2': {1: "Yes", 2: "No"},
        'DECIDE2': {1: "Never", 2: "Sometimes", 3: "Usually", 4: "Always"},
        'EXPLOP2': {1: "Yes", 2: "No"},
        'PROVTY2': {1: "Facility", 2: "Person", 3: "Person in Facility"},
        'LOCATN2': {1: "Office", 2: "Hospital, NO ER", 3: "Hospital ER"},
        'EMPST1': {1: "Yes", 2: "No"},
        'ADAPPT2': {1: "1", 2: "2", 3: "3", 4: "4", 5: "5-9", 6: "10 or more"},
        'REGION1': {1: "Northeast", 2: "Midwest", 3: "South", 4: "West"},
        'SEX': {1: "Male", 2: "Female"},
        'RACETHX': {1: "Hispanic", 2: "White", 3: "Black", 4: "Asian", 5: "Other"},
        'MARRY1X': {1: "Married", 2: "Widowed,Divorced,Seperated", 3: "Widowed,Divorced,Seperated",
                    4: "Widowed,Divorced,Seperated", 5: "Never Married", 6: "Under 16",
                    7: "Married", 8: "Widowed,Divorced,Seperated", 9: "Widowed,Divorced,Seperated",
                    10: "Widowed,Divorced,Seperated"},
        'INSCOVY1': {1: "Any Private", 2: "Public Only", 3: "Uninsured"},
        'POVCATY1': {1: "Poor/Negative", 2: "Near Poor", 3: "Low Income",
                     4: "Middle Income", 5: "High Income"},
        'OTHLANG': {1: "Yes", 2: "No"},
        'MNHLTH1': {1: "Excellent", 2: "Very Good", 3: "Good", 4: "Fair", 5: "Poor"},
        'HIDEG': {1: "Less than High School", 2: "High School", 3: "High School",
                  4: "Some College or Higher", 5: "Some College or Higher",
                  6: "Some College or Higher", 7: "Some College or Higher",
                  8: "Some College or Higher"}
    }
    for col, mapping in recoding_maps.items():
        df[col] = df[col].map(mapping)

    # --- Recode PPC items and create composite score ---
    ppc_items = ['ADLIST2', 'ADEXPL2', 'ADRESP2', 'ADPRTM2']
    for item in ppc_items:
        df[item] = np.select(
            [df[item].isin([1, 2]), df[item] == 3, df[item] == 4],
            [1, 2, 3],
            default=np.nan
        )
    df['PPC'] = df[ppc_items].sum(axis=1)
    df['PPC_cat'] = np.select(
        [df['PPC'] < 8, df['PPC'].between(8, 11), df['PPC'] == 12],
        ["Poor PPC", "Average PPC", "Perfect PPC"],
        default=None
    )

    # --- Winsorize outcome variables at the 99th percentile ---
    outcomes_to_winsorize = ['ERTOTY2', 'TOTTCHY2', 'OBDRVY2', 'RXEXPY2', 'RXTOTY2']
    for var in outcomes_to_winsorize:
        upper_bound = df[var].quantile(0.99)
        df[var] = np.where(df[var] > upper_bound, upper_bound, df[var])

    return df

# --- 3. MAIN DATA PROCESSING EXECUTION ---

# Process each yearly file and store it in a list
all_dfs = []
all_dfs.append(process_meps_year(2016))
all_dfs.append(process_meps_year(2017))
all_dfs.append(process_meps_year(2018))
all_dfs.append(process_meps_year(2019, column_map=COLUMN_NAME_MAP_2019))

# Combine all yearly dataframes into one
combined_data = pd.concat(all_dfs, ignore_index=True)

# Save the cleaned, combined data to a new CSV
combined_data.to_csv(f"{OUTPUT_PATH}combined_data_python.csv", index=False)
print("\nCombined and cleaned data saved to combined_data_python.csv")


# --- 4. STATISTICAL ANALYSIS ---

def run_meps_analysis(df):
    """
    Performs the full statistical analysis pipeline on the combined data.
    """
    print("\n--- Starting Statistical Analysis ---")

    # --- Step 4a: Prepare data for modeling ---
    # Create binary dummies for outcomes
    for outcome in ['OBDRVY2', 'RXTOTY2', 'RXEXPY2', 'ERTOTY2', 'TOTTCHY2']:
        df[f'{outcome}_0'] = np.where(df[outcome] > 0, 1, 0)

    # Create numeric versions of categorical variables for modeling
    cats_to_encode = ['SEX', 'MARRY1X', 'RACETHX', 'HIDEG', 'POVCATY1',
                      'INSCOVY1', 'RTHLTH1', 'MNHLTH1', 'REGION1', 'PPC_cat']
    for cat in cats_to_encode:
        df[f'{cat}_NUM'] = pd.Categorical(df[cat]).codes

    # Define survey weights
    weights = df['LONGWT']

    # --- Step 4b: Descriptive Statistics ---
    print("\n--- Generating Descriptive Statistics ---")
    # Weighted means for binary outcomes
    for var in ['OBDRVY2_0', 'RXTOTY2_0', 'RXEXPY2_0', 'ERTOTY2_0', 'TOTTCHY2_0']:
        w_stats = DescrStatsW(df[var], weights=weights, ddof=0)
        print(f"\nWeighted Mean of {var} by PPC Category:\n", df.groupby('PPC_cat').apply(lambda x: DescrStatsW(x[var], weights=x['LONGWT']).mean))

    # Weighted means for continuous outcomes
    for var in ['OBDRVY2', 'RXTOTY2', 'RXEXPY2', 'DSCONF3', 'ERTOTY2', 'TOTTCHY2', 'AGE1X']:
        w_stats = DescrStatsW(df[var], weights=weights, ddof=0)
        print(f"\nWeighted Mean of {var} by PPC Category:\n", df.groupby('PPC_cat').apply(lambda x: DescrStatsW(x[var], weights=x['LONGWT']).mean))

    # --- Step 4c: Regression Modeling ---
    print("\n--- Running Regression Models ---")
    # Define model components
    outcomes = {
        'RXEXPY2': {'family': sm.families.Gamma(link=sm.families.links.log()), 'twopm_fam': sm.families.Gamma(link=sm.families.links.log())},
        'RXTOTY2': {'family': sm.families.NegativeBinomial(link=sm.families.links.log()), 'twopm_fam': sm.families.NegativeBinomial(link=sm.families.links.log())},
        'OBDRVY2': {'family': sm.families.NegativeBinomial(link=sm.families.links.log()), 'twopm_fam': sm.families.NegativeBinomial(link=sm.families.links.log())},
        'ERTOTY2': {'family': sm.families.NegativeBinomial(link=sm.families.links.log()), 'twopm_fam': sm.families.NegativeBinomial(link=sm.families.links.log())},
        'TOTTCHY2': {'family': sm.families.Gamma(link=sm.families.links.log()), 'twopm_fam': sm.families.Gamma(link=sm.families.links.log())}
    }
    # Using C() to treat numeric variables as categorical in the formula
    controls = "C(SEX_NUM) + AGE1X + C(RACETHX_NUM) + C(MARRY1X_NUM) + C(HIDEG_NUM) + C(POVCATY1_NUM) + C(INSCOVY1_NUM) + C(RTHLTH1_NUM) + C(MNHLTH1_NUM) + C(REGION1_NUM)"
    
    for y_var, modelspec in outcomes.items():
        print(f"\n\n--- MODELS FOR OUTCOME: {y_var} ---")
        formula = f"{y_var} ~ C(PPC_cat_NUM, Treatment(1)) + {controls}" # Average PPC is reference

        # OLS Model
        print(f"\n-- OLS Results for {y_var} --")
        ols_model = smf.wls(formula, data=df, weights=weights).fit()
        print(ols_model.summary())

        # GLM Model
        print(f"\n-- GLM Results for {y_var} --")
        glm_model = smf.glm(formula, data=df, family=modelspec['family'], freq_weights=weights).fit()
        print(glm_model.summary())

        # Two-Part Model
        print(f"\n-- Two-Part Model Results for {y_var} --")
        # Part 1: Probit for having a non-zero outcome
        probit_formula = f"{y_var}_0 ~ C(PPC_cat_NUM, Treatment(1)) + {controls}"
        probit_model = smf.probit(probit_formula, data=df, freq_weights=weights).fit()
        print("\n-- Two-Part Model: Part 1 (Probit) --")
        print(probit_model.summary())
        
        # Part 2: GLM on positive values only
        df_positive = df[df[y_var] > 0].copy()
        weights_positive = df_positive['LONGWT']
        glm_part2_model = smf.glm(formula, data=df_positive, family=modelspec['twopm_fam'], freq_weights=weights_positive).fit()
        print("\n-- Two-Part Model: Part 2 (GLM on Positive Values) --")
        print(glm_part2_model.summary())

    # --- Logistic Regression for DSCONF3 ---
    print("\n\n--- LOGISTIC REGRESSION FOR DSCONF3 ---")
    logistic_formula = f"DSCONF3 ~ C(PPC_cat_NUM, Treatment(1)) + {controls}"
    logit_model = smf.logit(logistic_formula, data=df, freq_weights=weights).fit()
    print(logit_model.summary())
    # Print odds ratios
    print("\nOdds Ratios for DSCONF3 Model:")
    print(np.exp(logit_model.params))


# --- 5. EXECUTE ANALYSIS ---
run_meps_analysis(combined_data)

print("\n\n--- End of Script ---")
